# Awesome-MM-LLM

## Multimodal Large Language Models

  - [Surveys](#surveys)
  - [Vision](#vision)
  - [Audios](#audios)
  - [Any-to-Any](#any-to-any)
  - [MM-LLM with Robotics](#mm-llm-with-robotics)
  - [Datasets](#datasets)

---
## Surveys

* **[MM-LLM]** "A Survey on Multimodal Large Language Models", *arXiv, June 2023*.
[[Paper](https://arxiv.org/pdf/2306.13549.pdf)] [[Website](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)]
* **[LAM]** "Sparks of Large Audio Models:A Survey and Outlook", *arXiv, Sep 2023*.
[[Paper](https://arxiv.org/pdf/2308.12792.pdf)] [[Website](https://github.com/EmulationAI/awesome-large-audio-models)]


---
## Vision

* **[CLIP]** "Learning transferable visual models from natural language supervision", *arXiv, Feb 2021*.
[[Paper](https://arxiv.org/abs/2103.00020)] [[Website](https://github.com/openai/CLIP)]
* **[BLIP]** "BLIP: Bootstrapping Language-Image Pre-training for
Unified Vision-Language Understanding and Generation", *arXiv, Jan 2022*.
[[Paper](https://arxiv.org/abs/2201.12086)] [[Website](https://github.com/salesforce/BLIP)]
* **[BLIP-2]** "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", *arXiv, Jan 2022*.
[[Paper](https://arxiv.org/pdf/2301.12597.pdf)] [[Website](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)]
* **[MiniGPT-4]** "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models
", *arXiv, Apr 2023*.
[[Paper](https://arxiv.org/abs/2304.10592)] [[Website](https://minigpt-4.github.io/)]



---
## Audios

* **[AudioGPT]** "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head", *arXiv, Apr 2023*.
[[Paper](https://arxiv.org/abs/2304.12995)] [[Website](https://github.com/AIGC-Audio/AudioGPT)]


---
## Any-to-Any
* **[NExT-GPT]** "NExT-GPT: Any-to-Any Multimodal LLM", *arXiv, Sep 2023*.
[[Paper](https://arxiv.org/pdf/2309.05519.pdf)] [[Website](https://next-gpt.github.io/)]



---
## MM-LLM with Robotics

* **[PaLM-E]** "PaLM-E: An Embodied Multimodal Language Model", *arXiv, Feb 2021*.
[[Paper](https://arxiv.org/abs/2303.03378)] [[Website](https://palm-e.github.io/)]

---
## Datasets

